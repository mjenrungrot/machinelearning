\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage{graphicx}
\RequirePackage[12tabu, orthodox]{nag}
\usepackage{microtype} 	% impoves spacing
\usepackage{siunitx}	% simplify TeXing
\usepackage{cleveref}
\usepackage[colorlinks=false, pdfborder = {0 0 0}]{hyperref}

% Table of contents section
% -1 part     1 section     3 subsubsection  5 subparagraph
%  0 chapter  2 subsection  4 paragraph
\setcounter{tocdepth}{3}

% Automatic parenthesizing
% \usepackage{nath}
% \delimgrowth=1

% pseudocode
\usepackage{algorithmicx}
\usepackage{algpseudocode}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% TITLE 
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Machine Learning}
\author{Herrick Fang}
\date{ }

\begin{document}
\maketitle

\tableofcontents

\section{Machine Learning vs Statistical Learning}
Machine Learning: applies more toward large scale applications focusing on prediction and accuracy
\\
Statistical Learning: focuses more on models, interpretation, precision, and uncertainty

\section{Supervised Learning}
The algorithms receive ``Right Answers.''   

\subsection{Linear Regression with One Variable}
Linear regressions is one of the most fundamental tools for machine learning. It predicts real continuous valued output in a finite ordered set (e.g. survived, digit 0-9, class).

\subsubsection{Model and Cost Function}
The goal of this model is to minimize $\theta_{i}'s$, the {\bf parameters}, such that the hypothesis function,

\begin{equation}
     \label{eq:hypothesis_function}
       h(x) = \sum_{i=0}^{n}\theta_i x_i = \theta^Tx
\end{equation}

is close to $y$ given the input variables $x's$, so that our set of $m$ number of training examples  $\left( x^{\left( i \right)}, y^{\left( i \right)} \right) $, will be minimized.

\paragraph{Cost Function}
The squared error function is a reasonable choice in minimizing the hypothesis function,

\begin{equation}
    \label{eq:cost_function}
    J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2.
\end{equation}

where we aim to minimize $\theta_{0}$ and $\theta_{1}$. 

A way to visualize the cost function is a bow-shaped function or a contour plot. 

\begin{center}
\includegraphics[width = 2.5in, height = 2.5in]{bowplot}
\includegraphics[width = 5in, height = 2.5in]{contour}
\end{center}

\paragraph{Gradient Descent}
The algorithm starts with a set $\theta_{0}$ and $\theta_{1}$ then these values keep changing to reduce $J\left( \theta_{0}, \theta_{1} \right)$ until we end up at a minimum. Essentially, we try to find a way to walk down a hill in the fastest way possible by looking around and choosing the farthest step.

The {\bf gradient descent} algorithm starts with some initial $\theta$, and repeats
\begin{equation}
	\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta).
\end{equation} 
where $\alpha$ is the {\bf learning rate} until convergence. 

To implement this algorithm correctly, we must simultaneously update by assigning {\it ALL} dummy variables to the values of $\theta_j$ before assigning the new $\theta_j$ to the dummy variables.

Let's work out one update of one training example $(x,y)$. We have

\begin{align*}
	\frac{\partial}{\partial\theta_j}J(\theta) &= \frac{\partial}{\partial\theta_j}\frac{1}{2(1)}(h_\theta(x)-y)^2 \\
	&= 2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\
	&= (h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}\left(\sum_{i=0}^{n}\theta_ix_i-y\right) \\
	&= (h_\theta(x)-y)x_j
\end{align*}

Some intuition includes knowing that a positive derivative leads to a decrease in $\theta_j$ and vice versa.

This essentially means that if $\alpha$ is too small, gradient descent can be slow and if $\alpha$ is too large, gradient descent can overshoot the minimum, which means that it fails to converge.


\subsection{Classification}
Discrete/Quantitative Outputs Only (e.g. price, blood pressure)

\section{Unsupervised Learning}
We are not telling the algorithm the right answer, so the computer finds its own patterns.


\end{document}